---
title: "Global Clustering Based On Time Series Features"
output:
  html_notebook:
    toc: true
---

Clustering is a common unsupervised learning approach. The problem with time series
is that the notion of "distance" between time series isn't as straight forward as it
is for independent multidimensional data points. Common distance metrics like
euclidean or Manhattan distance don't necessarily tell you about how close two different
time series are and in many cases the results of those computations would go against
the human intuition. A way to still use clustering algorithms (and classification
algorithms for that matter) on time series data sets is to compute time series
features - metrics like the first local extreme points, points below the mean, etc -
and use all of those features in place of the original (univariate) time series.

In this document we try to apply this method on the stock data we collected in
the data acquisition document. We will try to find meaningful clusters of tickers,
look at their composition (which kind of companies can be found in these clusters)
and at differences in their performance.

```{r setup, include = FALSE}
library(tidyverse)
library(tsibble)
library(feasts)
library(Rcatch22)
library(janitor)

ticker_data <- read_csv("data/ticker_values.csv",
                        col_types = cols(
                          price.open = col_double(),
                          price.high = col_double(),
                          price.low = col_double(),
                          price.close = col_double(),
                          volume = col_double(),
                          price.adjusted = col_double(),
                          ref.date = col_date(format = ""),
                          ticker = col_character(),
                          ret.adjusted.prices = col_double(),
                          ret.closing.prices = col_double()
                        )) %>%
  as_tsibble(key = ticker, index = ref.date)

ticker_meta <- read_csv(
  "data/ticker_meta.csv",
  col_types = cols(
    .default = col_character(),
    Round.Lot.Size = col_double()
)) %>%
  filter(Name != "Merck & Company, Inc. Common Stock Ex-Distribution When Issued")
```

## Data Preparation

The chosen tickers currently aren't on the same scale. We can remedy this by normalizing
all values for each symbol. We'll focus on the adjusted price which is the closing
price after corrections for splits and dividend distributions.

```{r}
prices <- ticker_data %>%
  select(price.adjusted, ticker) %>%
  fill_gaps() %>%
  fill(price.adjusted, .direction = "down") %>%
  group_by(ticker) %>%
  mutate(standardized_price = (price.adjusted - mean(price.adjusted)) / sd(price.adjusted)) %>%
  ungroup() %>%
  select(-price.adjusted)

prices
```

Looking at all 216 symbols we can see similarities regarding big economic trends
(bang/bust cycles and long time upward trend) as well as individual differences
(a lot of steady values compared to more volatile stocks). We expect the chosen
stocks to show some common characteristics as the selection criteria are rather
stringent. Each title had to be part of the S&P 500 at one point in time during
the last 30 years while also having data from 1999 to 2021. This favors established
companies over new titles (left censoring) while also favoring extremely vetted
stocks over the general population of traded titles (biased sample).

```{r}
prices %>%
  ggplot(aes(x = ref.date, standardized_price, group = ticker)) +
  geom_line()
```

## Time Series Feature Computation

Working with raw time series values is both computationally expensive as well as
generally hard to do right. To circumvent these problems researchers of all fields
that use time series data have been doing extensive work to find features which
represent time series as a vector of comparable features. One of those efforts
culminated into the
[Highly comparative time-series analysis package](https://github.com/benfulcher/hctsa)
which allows to compute over 7000 features. Based on this work the researchers
behind the [catch22 package](https://github.com/chlubba/catch22) selected 22
features which - on average - offer the best cost/benefit trade-off in regarding
general time series clustering and classification tasks. As we do not have
in-depth knowledge of econometrics we decided to go with the `catch22` set of
features rather than hand-picking the best set of features ourselves.

```{r}
features_for_price_range <- function(price_range) {
  price_range %>%
    features(standardized_price,
             list(~ catch22_all(.) %>% pivot_wider(names_from = names, values_from = values)))
}

c22_price_feats <- features_for_price_range(prices)
c22_price_feats
```

```{r, include = FALSE}
library(tidymodels)
```

Similar to "traditional" clustering and classification tasks we decided to normalize
the features to bring them on the same scale.

```{r}
prep_rec <- recipe(~ ., data = c22_price_feats) %>%
  step_normalize(all_numeric()) %>%
  prep()


prepare_price_range <- function(featurized_prices, .rec) {
  .rec %>%
    bake(featurized_prices) %>%
    column_to_rownames("ticker") %>%
    # remove nulls because two columns are almost always constant for all observations
    select(-c(IN_AutoMutualInfoStats_40_gaussian_fmmi, SB_BinaryStats_diff_longstretch0)) %>%
    as.data.frame()
}

prepped_data <- c22_price_feats %>%
  prepare_price_range(.rec = prep_rec)
```

## K-Means Clustering

Without any indicators about the "right" amount of clusters and no prior knowledge
in econometrics or trading we decided to try k-means clustering to see if there
are obvious clusters within our data set.

```{r}
set.seed(1337)

centers <- c()
withinsss <- c()

for (center in 1:15) {
  kclust <- kmeans(prepped_data, centers = center, nstart = 10)
  twss <- kclust %>% glance() %>% (function (x) x$tot.withinss)
  
  centers[center] <- center
  withinsss[center] <- twss
}

kclust_res <- tibble(Centers = centers, TotalWithinss = withinsss)
kclust_res
```

To get a rough estimate about the number of valid clusters we computed clusters
with 1 to 15 centroids and compared the `withinss` measure. A visual analysis shows
the anticipated elbow at $k=2$.

```{r}
ggplot(kclust_res, aes(x = Centers, y = TotalWithinss)) +
  geom_point() +
  geom_line()
```

Creating a cluster with two centroids shows a rather unbalanced distribution of
170 versus 46 cluster members.

```{r}
opt_clust <- kmeans(prepped_data, centers = 2, nstart = 10)

cluster_sizes <- opt_clust %>% tidy() %>% (function (x) x$size) # your cluster sizes
cluster_sizes
```

As we don't see the temporal patterns in the feature-vector representation we join
the cluster labels with our normalized price dataset and plot the time series
in their respective clusters. We still see the the cycle and trend components in
both clusters while also getting the sense of a possible sensible partition between
stocks with certain high volatility behavior and stocks that show less movement
in their price development.

```{r, fig.width=10}
clustered_data <- augment(opt_clust, prepped_data) %>%
  rename(ticker = .rownames)

join_cluster_with_price_range <- function(.clustered, .price_range) {
  .price_range %>%
    left_join(.clustered %>% select(ticker, .cluster), by = "ticker")
}

plot_clustered_tickers <- function(.clustered) {
  nrow <- .clustered$.cluster %>%
    unique() %>%
    length()
  
  ggplot(.clustered, aes(x = ref.date, standardized_price, group = ticker, color = .cluster)) +
    geom_line() +
    facet_wrap(~ .cluster, nrow = nrow)
}

clustered_data %>%
  join_cluster_with_price_range(.price_range = prices) %>%
  plot_clustered_tickers()
```

We were able to collect additional metadata containing information about the business
sectors of certain titles within our data set. Looking at this data, we are
particularly interested in the relative composition of our clusters. As we can
see in the resulting figure there are striking differences but unfortunately
the fraction of "unknown" sectors in the second cluster is far too high
to base any interpretation on this result.

```{r, fig.width=10}
enrich_with_meta <- function(clustered_prices) {
  clustered_prices %>%
    left_join(ticker_meta, by = c("ticker" = "Symbol")) %>%
    replace_na(list(Sector = "Not Specified", Subsector = "Not Specified")) 
}

sector_freq_per_cluster <- function(enriched_prices) {
  nrow <- enriched_prices$.cluster %>%
    unique() %>%
    length()
  
  enriched_prices %>%
    group_by(.cluster, Sector) %>%
    summarise(count = n()) %>%
    mutate(freq = count / sum(count)) %>%
    ggplot(aes(x = Sector, y = freq, color = Sector, fill = Sector)) +
    geom_col() +
    facet_wrap(~ .cluster, nrow = nrow) +
    coord_flip()
}

clustered_data %>%
  enrich_with_meta() %>%
  sector_freq_per_cluster()
```

### Further Dimensionality Reduction

In order to even further reduce the dimensions of our already rather slim
feature vectors we decided to try a primary component anlysis.

```{r}
pca_estimates <- recipe(~., data = prepped_data) %>%
  step_pca(all_numeric(), num_comp = NCOL(prepped_data)) %>%
  prep()

extract_pca_components <- function(.df, .rec) {
  components <- .rec %>%
    bake(.df) %>%
    as.data.frame()
  
  rownames(components) <- rownames(.df)
  components 
}

components <- extract_pca_components(prepped_data, pca_estimates)
components
```

Looking at the percentages of variation each component explains we can see that
the amount of explained variation rapidly decreases. This indicates, that we can
most likely only work on a subset of components without loosing a lot of information.

```{r}
stdev <- pca_estimates$steps[[1]]$res$sdev
percent_variation <- stdev ^ 2 / sum(stdev ^ 2)

variations <- tibble(PC = paste0("PC", 1:length(stdev)),
                     VarExplained = percent_variation)

explained <- 0
n_comps_for_095 <- 0
for (row in 1:NROW(variations)) {
  explained <- explained + variations$VarExplained[row]
  n_comps_for_095 <- n_comps_for_095 + 1
  if (explained >= 0.95) {
    break;
  }
}

variations %>%
  mutate(PC = forcats::fct_inorder(PC)) %>%
  # just take the first 20 because the rest doesn't matter too much anyway
  # and the plot gets very hard to read otherwise
  slice_head(n = n_comps_for_095) %>%
  ggplot(aes(x = PC, y = VarExplained)) +
  geom_col()
```

As `r n_comps_for_095` explain `r explained * 100`% of the variability in our data
set we can throw out the last `r 20 - n_comps_for_095` components. With this
additional preprocessing step in place we decided to repeat our initial k-means
cluster analysis which led to a very similar result. We take this as a further
indication that $k=2$ is valid.

```{r}
set.seed(1337)

pca_96_perc_rec <- recipe(~., data = prepped_data) %>%
  step_pca(all_numeric(), num_comp = 15) %>%
  prep()

reduced_components <- extract_pca_components(prepped_data, pca_96_perc_rec)

centers <- c()
withinsss <- c()

for (center in 1:15) {
  kclust <- kmeans(reduced_components, centers = center, nstart = 10)
  twss <- kclust %>% glance() %>% (function (x) x$tot.withinss)
  
  centers[center] <- center
  withinsss[center] <- twss
}

kclust_res <- tibble(Centers = centers, TotalWithinss = withinsss)

ggplot(kclust_res, aes(x = Centers, y = TotalWithinss)) +
  geom_point() +
  geom_line()
```

Looking only at the first and the second component we can see that there appears
pretty high certainty about the first cluster, which appears to be rather dense
while the second cluster has very fuzzy boundaries and less density.

```{r}
set.seed(1337)

pca_clust <- kmeans(reduced_components, nstart = 10, centers = 2)

clustered_components <- augment(pca_clust, components)

ggplot(clustered_components, aes(x = PC01, y = PC02, color = .cluster)) +
  geom_point(size = 2.5)
```

## Clustering via HDBSCAN

```{r}
library(dbscan)
set.seed(1337)

hdb <- dbscan::hdbscan(prepped_data, minPts = 4)
hdb
```

```{r}
plot(hdb, show_flat = TRUE)
```

```{r}
clustered_data <- bind_cols(prepped_data, list(.cluster = hdb$cluster)) %>%
  rownames_to_column("ticker")

clustered_data %>%
  join_cluster_with_price_range(.price_range = prices) %>%
  plot_clustered_tickers()
```

```{r}
set.seed(1337)

hdb <- dbscan::hdbscan(reduced_components, minPts = 6)
hdb
```

```{r}
plot(hdb, show_flat = TRUE)
```

```{r}
perform_clustering <- function(.ds, .minPts = 6) {
  set.seed(1337)
  hdb <- dbscan::hdbscan(reduced_components, minPts = .minPts)
  
  bind_cols(.ds, list(.cluster = hdb$cluster)) %>%
    rownames_to_column("ticker")
}

clustered_data <- reduced_components %>%
  perform_clustering()

clustered_data %>%
  join_cluster_with_price_range(.price_range = prices) %>%
  plot_clustered_tickers()
```

```{r, fig.width=10}
clustered_data %>%
  enrich_with_meta() %>%
  sector_freq_per_cluster()
```

```{r}
clustered_data %>%
  select(ticker, .cluster) %>%
  left_join(ticker_meta, by = c("ticker" = "Symbol")) %>%
  filter(.cluster == 1)
```

```{r}
get_tickers_for_cluster <- function(.clustered, .clust_num) {
  .clustered %>%
    filter(.cluster == .clust_num) %>%
    select(ticker) %>%
    (function(x) x$ticker)
}

clust_one_symbols <- get_tickers_for_cluster(clustered_data, 1)
clust_one_symbols
```

```{r}
library(tidyquant)

get_mreturns_per_ticker_in_range <- function(.tickers, .price_range) {
  min_date <- format(min(.price_range$ref.date), "%Y-%m-%d")
  max_date <- format(max(.price_range$ref.date), "%Y-%m-%d")
  
  ticker_data %>%
    filter(ticker %in% .tickers) %>%
    filter_index(min_date ~ max_date) %>%
    rename(date = ref.date) %>%
    tibble() %>%
    group_by(ticker) %>%
    tq_transmute(select = price.adjusted,
                 mutate_fun = periodReturn,
                 period = "monthly",
                 col_rename = "Ra")
}

Ra <- get_mreturns_per_ticker_in_range(clust_one_symbols, prices)
Ra
```

```{r}
get_baseline_mreturns_in_range <- function(.price_range) {
  min_date <- format(min(.price_range$ref.date), "%Y-%m-%d")
  max_date <- format(max(.price_range$ref.date), "%Y-%m-%d")
  
  "^GSPC" %>% # S&P 500 is baseline for the clusters
    tq_get(get = "stock.prices",
           from = min_date,
           to = max_date) %>%
    tq_transmute(select = adjusted,
                 mutate_fun = periodReturn,
                 period = "monthly",
                 col_rename = "Rb")
}

Rb <- get_baseline_mreturns_in_range(prices)
Rb
```

```{r}
RaRb <- left_join(Ra, Rb, by = "date")
RaRb
```

```{r}
ticker_performance_against_baseline_in_range <- function(.tickers, .price_range) {
  Ra <- get_mreturns_per_ticker_in_range(.tickers, .price_range)
  Rb <- get_baseline_mreturns_in_range(.price_range)
  RaRb <- left_join(Ra, Rb, by = "date")
  
  RaRb %>%
    tq_performance(Ra = Ra,
                   Rb = Rb,
                   performance_fun = table.CAPM)
}

RaRb_capm <- ticker_performance_against_baseline_in_range(clust_one_symbols, prices)
RaRb_capm
```

```{r}
RaRb_capm %>%
  ggplot(aes(x = ActivePremium)) +
  geom_histogram(binwidth = 0.005)
```

## Try out windowing for one 

```{r}
library(slider)

prices_index <- prices %>%
  arrange(ref.date) %>%
  (function(x) x$ref.date)

windowed_prices <- prices %>%
  tibble() %>%
  group_by(ticker) %>%
  arrange(ref.date) %>%
  slide_period(prices_index,
               "year",
               .f = (function(x) ungroup(x) %>% as_tsibble(index = ref.date, key = ticker)),
               .before = 2,
               .complete = TRUE)
```

```{r}
price_range_1999_2001 <- windowed_prices[[3]]

price_range_1999_2001 %>%
  ggplot(aes(x = ref.date, standardized_price, group = ticker)) +
  geom_line()
```

```{r}
test_window_prepped_data <- price_range_1999_2001 %>%
  features_for_price_range() %>%
  prepare_price_range(.rec = prep_rec) %>%
  extract_pca_components(pca_96_perc_rec)

test_window_prepped_data
```

```{r}
test_window_clustered_data <- test_window_prepped_data %>%
  perform_clustering()

test_window_clustered_data %>%
  join_cluster_with_price_range(.price_range = price_range_1999_2001) %>%
  plot_clustered_tickers()
```

```{r, fig.width=10}
test_window_clustered_data %>%
  enrich_with_meta() %>%
  sector_freq_per_cluster()
```

```{r}
performance_noise_cluster_test_window <- get_tickers_for_cluster(test_window_clustered_data, 0) %>%
  ticker_performance_against_baseline_in_range(price_range_1999_2001)

performance_noise_cluster_test_window
```

```{r}
performance_noise_cluster_test_window %>%
  ggplot(aes(x = ActivePremium)) +
  geom_histogram(binwidth = 0.01)
```

```{r}
get_tickers_for_cluster(test_window_clustered_data, 1) %>%
  ticker_performance_against_baseline_in_range(price_range_1999_2001) %>%
  ggplot(aes(x = ActivePremium)) +
  geom_histogram(binwidth = 0.01)
```

```{r}
get_tickers_for_cluster(test_window_clustered_data, 2) %>%
  ticker_performance_against_baseline_in_range(price_range_1999_2001) %>%
  ggplot(aes(x = ActivePremium)) +
  geom_histogram(binwidth = 0.01)
```

```{r}
cluster_price_range <- function(.price_range) {
  unclustered_ticker_graph <- .price_range %>%
    ggplot(aes(x = ref.date, standardized_price, group = ticker)) +
    geom_line()
  
  price_range_prepped_data <- .price_range %>%
    features_for_price_range() %>%
    prepare_price_range(.rec = prep_rec) %>%
    extract_pca_components(pca_96_perc_rec)
  
  price_range_clustered_data <- price_range_prepped_data %>%
    perform_clustering()

  clustered_ticker_graph <- price_range_clustered_data %>%
    join_cluster_with_price_range(.price_range = .price_range) %>%
    plot_clustered_tickers()
  
  clustered_sector_composition <- price_range_clustered_data %>%
    enrich_with_meta() %>%
    sector_freq_per_cluster()
  
  list(price_range = .price_range,
       unclustered_graph = unclustered_ticker_graph,
       clustered_data = price_range_clustered_data,
       clustered_graph = clustered_ticker_graph,
       clustered_sectors = clustered_sector_composition)
}

clustered_price_windows <- prices %>%
  tibble() %>%
  group_by(ticker) %>%
  arrange(ref.date) %>%
  slide_period(prices_index,
               "year",
               .f = (function(x) ungroup(x) %>%
                       as_tsibble(index = ref.date, key = ticker) %>%
                       cluster_price_range()),
               .before = 2,
               .complete = TRUE)
```

```{r}
clustered_price_windows[[4]]
```

```{r}
clustered_price_windows[[11]]
```

```{r}
clustered_price_windows[[13]]
```

```{r}
clustered_price_windows[[22]]
```

```{r}
label_for_price_range <- function(price_range) {
  min_year <- format(price_range$ref.date %>% min(), "%Y")
  max_year <- format(price_range$ref.date %>% max(), "%Y")
  paste(min_year, "_", max_year, sep = "")
}

extract_cluster_info <- function(window_results) {
  column_label <- label_for_price_range(window_results$price_range)
  
  window_results$clustered_data %>%
    select(ticker, .cluster) %>%
    rename_with(~ column_label, .cluster)  
}

extract_cluster_info(clustered_price_windows[[3]])
```

```{r}
cluster_membership_over_time <- clustered_price_windows %>%
  compact() %>%
  lapply(extract_cluster_info) %>%
  reduce(inner_join, by = "ticker")

cluster_membership_over_time
```

```{r}
cluster_membership_over_time %>%
  pivot_longer(cols = c(everything(), -ticker), names_to = "window", values_to = "cluster") %>%
  group_by(ticker) %>%
  summarise(memberships = length(unique(cluster))) %>%
  filter(memberships > 1)
```

```{r}
tickers_by_all_time_cluster <- cluster_membership_over_time %>%
  pivot_longer(cols = c(everything(), -ticker), names_to = "window", values_to = "cluster") %>%
  group_by(ticker) %>%
  summarise(all_time_cluster = cluster %>% first())

tickers_by_all_time_cluster
```

```{r}
get_cluster_performance <- function(window_result) {
  price_period <- label_for_price_range(window_result$price_range)
  
  window_result$clustered_data$.cluster %>%
    unique() %>%
    lapply((function(x) get_tickers_for_cluster(window_result$clustered_data, x) %>%
              ticker_performance_against_baseline_in_range(window_result$price_range) %>%
              (function(y) tibble(median_active_premium = median(y$ActivePremium, na.rm = TRUE),
                                  cluster = x,
                                  period = price_period)))) %>%
    reduce(bind_rows)
}

get_cluster_performance(clustered_price_windows[[3]])
```
```{r}
cluster_performances_per_period <- clustered_price_windows %>%
  compact() %>%
  lapply(get_cluster_performance) %>%
  reduce(bind_rows)

cluster_performances_per_period
```

```{r}
cluster_performances_per_period %>%
  mutate(cluster = as_factor(cluster)) %>%
  ggplot(aes(x = cluster, y = median_active_premium, fill = cluster)) +
  geom_violin()
```

```{r}
cluster_performances_per_period %>%
  filter(cluster %in% c(0, 2)) %>%
  (function(x) aov(cluster ~ median_active_premium, data = x)) %>%
  summary()
```


```{r}
cluster_of_interest <- prices %>%
  tibble() %>%
  left_join(tickers_by_all_time_cluster, by = "ticker") %>%
  left_join(ticker_meta, by = c("ticker" = "Symbol")) %>%
  filter(all_time_cluster == 2) %>%
  as_tsibble(index = ref.date, key = ticker) %>%
  remove_empty(which = "cols")

cluster_of_interest
```

```{r}
cluster_of_interest %>%
  ggplot(aes(x = ref.date, standardized_price, group = ticker)) +
  geom_line()
```

```{r}
tickers_by_all_time_cluster %>%
  filter(all_time_cluster == 2) %>%
  left_join(ticker_meta, by = c("ticker" = "Symbol")) %>%
  select(ticker, Name, Sector, Subsector)
```
