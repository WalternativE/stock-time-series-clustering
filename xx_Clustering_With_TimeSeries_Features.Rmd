---
title: "Global Clustering Based On Time Series Features"
output:
  html_notebook:
    toc: true
---

Clustering is a common unsupervised learning approach. The problem with time series
is that the notion of "distance" between time series isn't as straight forward as it
is for independent multidimensional data points. Common distance metrics like
euclidean or Manhattan distance don't necessarily tell you about how close two different
time series are and in many cases the results of those computations would go against
the human intuition. A way to still use clustering algorithms (and classification
algorithms for that matter) on time series data sets is to compute time series
features - metrics like the first local extreme points, points below the mean, etc -
and use all of those features in place of the original (univariate) time series.

In this document we try to apply this method on the stock data we collected in
the data acquisition document. We will try to find meaningful clusters of tickers,
look at their composition (which kind of companies can be found in these clusters)
and at differences in their performance.

```{r setup, include = FALSE}
library(tidyverse)
library(tsibble)
library(feasts)
library(Rcatch22)
library(janitor)
library(tidyquant)

ticker_data <- read_csv("data/ticker_values.csv",
                        col_types = cols(
                          price.open = col_double(),
                          price.high = col_double(),
                          price.low = col_double(),
                          price.close = col_double(),
                          volume = col_double(),
                          price.adjusted = col_double(),
                          ref.date = col_date(format = ""),
                          ticker = col_character(),
                          ret.adjusted.prices = col_double(),
                          ret.closing.prices = col_double()
                        )) %>%
  as_tsibble(key = ticker, index = ref.date)

ticker_meta <- read_csv(
  "data/ticker_meta.csv",
  col_types = cols(
    .default = col_character(),
    Round.Lot.Size = col_double()
)) %>%
  filter(Name != "Merck & Company, Inc. Common Stock Ex-Distribution When Issued")
```

## Data Preparation

The chosen tickers currently aren't on the same scale. We can remedy this by normalizing
all values for each symbol. We'll focus on the adjusted price which is the closing
price after corrections for splits and dividend distributions.

```{r}
prices <- ticker_data %>%
  select(price.adjusted, ticker) %>%
  fill_gaps() %>%
  fill(price.adjusted, .direction = "down") %>%
  group_by(ticker) %>%
  mutate(standardized_price = (price.adjusted - mean(price.adjusted)) / sd(price.adjusted)) %>%
  ungroup() %>%
  select(-price.adjusted)

prices
```

Looking at all 216 symbols we can see similarities regarding big economic trends
(bang/bust cycles and long time upward trend) as well as individual differences
(a lot of steady values compared to more volatile stocks). We expect the chosen
stocks to show some common characteristics as the selection criteria are rather
stringent. Each title had to be part of the S&P 500 at one point in time during
the last 30 years while also having data from 1999 to 2021. This favors established
companies over new titles (left censoring) while also favoring extremely vetted
stocks over the general population of traded titles (biased sample).

```{r}
prices %>%
  ggplot(aes(x = ref.date, standardized_price, group = ticker)) +
  geom_line()
```

## Time Series Feature Computation

Working with raw time series values is both computationally expensive as well as
generally hard to do right. To circumvent these problems researchers of all fields
that use time series data have been doing extensive work to find features which
represent time series as a vector of comparable features. One of those efforts
culminated into the
[Highly comparative time-series analysis package](https://github.com/benfulcher/hctsa)
which allows to compute over 7000 features. Based on this work the researchers
behind the [catch22 package](https://github.com/chlubba/catch22) selected 22
features which - on average - offer the best cost/benefit trade-off in regarding
general time series clustering and classification tasks. As we do not have
in-depth knowledge of econometrics we decided to go with the `catch22` set of
features rather than hand-picking the best set of features ourselves.

```{r}
features_for_price_range <- function(price_range) {
  price_range %>%
    features(standardized_price,
             list(~ catch22_all(.) %>% pivot_wider(names_from = names, values_from = values)))
}

c22_price_feats <- features_for_price_range(prices)
c22_price_feats
```

```{r, include = FALSE}
library(tidymodels)
```

Similar to "traditional" clustering and classification tasks we decided to normalize
the features to bring them on the same scale.

```{r}
prep_rec <- recipe(~ ., data = c22_price_feats) %>%
  step_normalize(all_numeric()) %>%
  prep()


prepare_price_range <- function(featurized_prices, .rec) {
  .rec %>%
    bake(featurized_prices) %>%
    column_to_rownames("ticker") %>%
    # remove nulls because two columns are almost always constant for all observations
    select(-c(IN_AutoMutualInfoStats_40_gaussian_fmmi, SB_BinaryStats_diff_longstretch0)) %>%
    as.data.frame()
}

prepped_data <- c22_price_feats %>%
  prepare_price_range(.rec = prep_rec)
```

## K-Means Clustering

Without any indicators about the "right" amount of clusters and no prior knowledge
in econometrics or trading we decided to try k-means clustering to see if there
are obvious clusters within our data set.

```{r}
set.seed(1337)

centers <- c()
withinsss <- c()

for (center in 1:15) {
  kclust <- kmeans(prepped_data, centers = center, nstart = 10)
  twss <- kclust %>% glance() %>% (function (x) x$tot.withinss)
  
  centers[center] <- center
  withinsss[center] <- twss
}

kclust_res <- tibble(Centers = centers, TotalWithinss = withinsss)
kclust_res
```

To get a rough estimate about the number of valid clusters we computed clusters
with 1 to 15 centroids and compared the `withinss` measure. A visual analysis shows
the anticipated elbow at $k=2$.

```{r}
ggplot(kclust_res, aes(x = Centers, y = TotalWithinss)) +
  geom_point() +
  geom_line()
```

Creating a cluster with two centroids shows a rather unbalanced distribution of
170 versus 46 cluster members.

```{r}
opt_clust <- kmeans(prepped_data, centers = 2, nstart = 10)

cluster_sizes <- opt_clust %>% tidy() %>% (function (x) x$size) # your cluster sizes
cluster_sizes
```

As we don't see the temporal patterns in the feature-vector representation we join
the cluster labels with our normalized price dataset and plot the time series
in their respective clusters. We still see the the cycle and trend components in
both clusters while also getting the sense of a possible sensible partition between
stocks with certain high volatility behavior and stocks that show less movement
in their price development.

```{r, fig.width=10}
clustered_data <- augment(opt_clust, prepped_data) %>%
  rename(ticker = .rownames)

join_cluster_with_price_range <- function(.clustered, .price_range) {
  .price_range %>%
    left_join(.clustered %>% select(ticker, .cluster), by = "ticker")
}

plot_clustered_tickers <- function(.clustered) {
  nrow <- .clustered$.cluster %>%
    unique() %>%
    length()
  
  ggplot(.clustered, aes(x = ref.date, standardized_price, group = ticker, color = .cluster)) +
    geom_line() +
    facet_wrap(~ .cluster, nrow = nrow)
}

clustered_data %>%
  join_cluster_with_price_range(.price_range = prices) %>%
  plot_clustered_tickers()
```

We were able to collect additional metadata containing information about the business
sectors of certain titles within our data set. Looking at this data, we are
particularly interested in the relative composition of our clusters. As we can
see in the resulting figure there are striking differences but unfortunately
the fraction of "unknown" sectors in the second cluster is far too high
to base any interpretation on this result.

```{r, fig.width=10}
enrich_with_meta <- function(clustered_prices) {
  clustered_prices %>%
    left_join(ticker_meta, by = c("ticker" = "Symbol")) %>%
    replace_na(list(Sector = "Not Specified", Subsector = "Not Specified")) 
}

sector_freq_per_cluster <- function(enriched_prices) {
  nrow <- enriched_prices$.cluster %>%
    unique() %>%
    length()
  
  enriched_prices %>%
    group_by(.cluster, Sector) %>%
    summarise(count = n()) %>%
    mutate(freq = count / sum(count)) %>%
    ggplot(aes(x = Sector, y = freq, color = Sector, fill = Sector)) +
    geom_col() +
    facet_wrap(~ .cluster, nrow = nrow) +
    coord_flip()
}

clustered_data %>%
  enrich_with_meta() %>%
  sector_freq_per_cluster()
```

### Further Dimensionality Reduction

In order to even further reduce the dimensions of our already rather slim
feature vectors we decided to try a primary component anlysis.

```{r}
pca_estimates <- recipe(~., data = prepped_data) %>%
  step_pca(all_numeric(), num_comp = NCOL(prepped_data)) %>%
  prep()

extract_pca_components <- function(.df, .rec) {
  components <- .rec %>%
    bake(.df) %>%
    as.data.frame()
  
  rownames(components) <- rownames(.df)
  components 
}

components <- extract_pca_components(prepped_data, pca_estimates)
components
```

Looking at the percentages of variation each component explains we can see that
the amount of explained variation rapidly decreases. This indicates, that we can
most likely only work on a subset of components without loosing a lot of information.

```{r}
stdev <- pca_estimates$steps[[1]]$res$sdev
percent_variation <- stdev ^ 2 / sum(stdev ^ 2)

variations <- tibble(PC = paste0("PC", 1:length(stdev)),
                     VarExplained = percent_variation)

explained <- 0
n_comps_for_095 <- 0
for (row in 1:NROW(variations)) {
  explained <- explained + variations$VarExplained[row]
  n_comps_for_095 <- n_comps_for_095 + 1
  if (explained >= 0.95) {
    break;
  }
}

variations %>%
  mutate(PC = forcats::fct_inorder(PC)) %>%
  # just take the first 20 because the rest doesn't matter too much anyway
  # and the plot gets very hard to read otherwise
  slice_head(n = n_comps_for_095) %>%
  ggplot(aes(x = PC, y = VarExplained)) +
  geom_col()
```

As `r n_comps_for_095` explain `r explained * 100`% of the variability in our data
set we can throw out the last `r 20 - n_comps_for_095` components. With this
additional preprocessing step in place we decided to repeat our initial k-means
cluster analysis which led to a very similar result. We take this as a further
indication that $k=2$ is valid.

```{r}
set.seed(1337)

pca_96_perc_rec <- recipe(~., data = prepped_data) %>%
  step_pca(all_numeric(), num_comp = 15) %>%
  prep()

reduced_components <- extract_pca_components(prepped_data, pca_96_perc_rec)

centers <- c()
withinsss <- c()

for (center in 1:15) {
  kclust <- kmeans(reduced_components, centers = center, nstart = 10)
  twss <- kclust %>% glance() %>% (function (x) x$tot.withinss)
  
  centers[center] <- center
  withinsss[center] <- twss
}

kclust_res <- tibble(Centers = centers, TotalWithinss = withinsss)

ggplot(kclust_res, aes(x = Centers, y = TotalWithinss)) +
  geom_point() +
  geom_line()
```

Looking only at the first and the second component we can see that there appears
pretty high certainty about the first cluster, which appears to be rather dense
while the second cluster has very fuzzy boundaries and less density.

```{r}
set.seed(1337)

pca_clust <- kmeans(reduced_components, nstart = 10, centers = 2)

clustered_components <- augment(pca_clust, components)

ggplot(clustered_components, aes(x = PC01, y = PC02, color = .cluster)) +
  geom_point(size = 2.5)
```

## Clustering via HDBSCAN

The problem with k-means (and many other clustering algorithms) is, that regardless
of whether clusters are valid or not it will form exactly as many clusters as there
are configured centroids. DBSCAN and HDBSCAN (a hierarchical clustering algorithm
based on DBSCAN) are alternative algorithms that - in addition to finding clusters
and classifying observations accordingly - can mark observations as outliers if
they don't fit any cluster. Using HDBSCAN on our normalized data set (without PCA)
would yield two clusters, one containing the majority of all observations and one
containing only four time series. The rest - 60 time series in total - would be
classified as "too fringe" to be part of any cluster.

```{r}
library(dbscan)
set.seed(1337)

hdb <- dbscan::hdbscan(prepped_data, minPts = 4)
hdb
```

As HDBSCAN is a hierarchical algorithm we can look at its dendrogram. Given that
we configured a minimum cluster size of 4 observations the smallest found cluster
contains exactly 4 observations.

```{r}
plot(hdb, show_flat = TRUE)
```

In contrast to the k-means clustering we see that the time series clustered together
have to be far more similar to actually land into the same cluster.

```{r}
clustered_data <- bind_cols(prepped_data, list(.cluster = hdb$cluster)) %>%
  rownames_to_column("ticker")

clustered_data %>%
  join_cluster_with_price_range(.price_range = prices) %>%
  plot_clustered_tickers()
```

### The Effects of Dimensionality Reduction on HDBSCAN

As we did in the k-means experiment we also decided to use the HDBSCAN algorithm
on the data set we reduced by performing a PCA on the normalized feature vectors.
As we can see this preprocessing allowed us to increase the minimum size of observations
per cluster while still getting decently sized clusters. As a trade-off the amount
of outlier observations dramatically increased. From our point it is hard to say
what would make more sense in regard to the financial domain. From our statistical
intuition we would assume that a bigger minimum of observations per cluster should
result in more valid clusters. Because of this we will use dimensionality reduction
in the rest of the analysis.

```{r}
set.seed(1337)

hdb <- dbscan::hdbscan(reduced_components, minPts = 6)
hdb
```

As before we can also inspect the dendrogram, which appears to be flatter and more
"balanced" than in our first HDBSCAN experiment.

```{r}
plot(hdb, show_flat = TRUE)
```

As for the clusters there appears to be an even higher similarity between the
time series in each cluster.

```{r}
perform_clustering <- function(.ds, .minPts = 6) {
  set.seed(1337)
  hdb <- dbscan::hdbscan(.ds, minPts = .minPts)
  
  bind_cols(.ds, list(.cluster = hdb$cluster)) %>%
    rownames_to_column("ticker")
}

clustered_data <- reduced_components %>%
  perform_clustering()

clustered_data %>%
  join_cluster_with_price_range(.price_range = prices) %>%
  plot_clustered_tickers()
```

Looking at the relative composition of each cluster we can see a more interesting
results than before. The outlier observations as well as the first cluster have
around 20% to 25% tickers of not closer specified business sectors in them (the
sample has `r (sum(is.na(ticker_meta$Sector)) / NROW(ticker_meta$Sector)) * 100`%
unspecified values) while the second cluster only has around 5% not closer specified
symbols.

```{r, fig.width=10}
clustered_data %>%
  enrich_with_meta() %>%
  sector_freq_per_cluster()
```

In comparison to the relative composition of the complete sample we can see that
the disperse group of outlier observations is relatively similar while the two
different clusters are distinctly different.

```{r, fig.width=10}
ticker_meta %>%
  group_by(Sector) %>%
  summarise(count = n()) %>%
  mutate(freq = count / sum(count)) %>%
  ggplot(aes(x = Sector, y = freq, color = Sector, fill = Sector)) +
  geom_col() +
  coord_flip()
```

## Exploring Differences in (Financial) Cluster Performance

As we can see above we can already see distinct differences in business sectors
which contribute to each cluster. Another interesting piece of additional
information for us would be if the clusters we found perform differently from their
base index: the S&P 500.

To compare the differences we make use of the `tidyquant` library, a package that
includes common functionality used by "quants" (quantitative financial analysts).
One of these common functionalities is to calculate the return over a period
(in this case monthly) for a set of assets. In the following example we calculate
the monthly returns of all tickers in our biggest cluster.

```{r}
get_tickers_for_cluster <- function(.clustered, .clust_num) {
  .clustered %>%
    filter(.cluster == .clust_num) %>%
    select(ticker) %>%
    (function(x) x$ticker)
}

clust_two_symbols <- get_tickers_for_cluster(clustered_data, 2)

get_mreturns_per_ticker_in_range <- function(.tickers, .price_range) {
  min_date <- format(min(.price_range$ref.date), "%Y-%m-%d")
  max_date <- format(max(.price_range$ref.date), "%Y-%m-%d")
  
  ticker_data %>%
    filter(ticker %in% .tickers) %>%
    filter_index(min_date ~ max_date) %>%
    rename(date = ref.date) %>%
    tibble() %>%
    group_by(ticker) %>%
    tq_transmute(select = price.adjusted,
                 mutate_fun = periodReturn,
                 period = "monthly",
                 col_rename = "Ra")
}

Ra <- get_mreturns_per_ticker_in_range(clust_two_symbols, prices)
Ra
```

In order to see if our cluster of interest performs better or worse, than the
index we have to first retrieve the same returns for the S&P 500. The symbol in
question would be `^GSPC`.

```{r}
get_baseline_mreturns_in_range <- function(.price_range) {
  min_date <- format(min(.price_range$ref.date), "%Y-%m-%d")
  max_date <- format(max(.price_range$ref.date), "%Y-%m-%d")
  
  "^GSPC" %>% # S&P 500 is baseline for the clusters
    tq_get(get = "stock.prices",
           from = min_date,
           to = max_date) %>%
    tq_transmute(select = adjusted,
                 mutate_fun = periodReturn,
                 period = "monthly",
                 col_rename = "Rb")
}

Rb <- get_baseline_mreturns_in_range(prices)
Rb
```

We can use `tidyquant` to calculate the performance using the monthly returns
of our cluster assets as well as the baseline. Unfortunately we would need access
to real domain experts for a thorough analysis of the results. As we are not
"quants" ourselves we focus on the most intuitive value in the result data set
which would be the active premium. The active premium is calculated by subtracting
the baseline annualized returns from the asset annualized returns. So - being very
reductive - a higher active premium indicates a better performance.

```{r}
ticker_performance_against_baseline_in_range <- function(.tickers, .price_range) {
  Ra <- get_mreturns_per_ticker_in_range(.tickers, .price_range)
  Rb <- get_baseline_mreturns_in_range(.price_range)
  RaRb <- left_join(Ra, Rb, by = "date")
  
  RaRb %>%
    tq_performance(Ra = Ra,
                   Rb = Rb,
                   performance_fun = table.CAPM)
}

RaRb_capm <- ticker_performance_against_baseline_in_range(clust_two_symbols, prices)
RaRb_capm
```

If we look at the active premium distribution for our cluster assets we can see
that the median active premium is `r median(RaRb_capm$ActivePremium) * 100`% while
the whole distribution generally stayes to the right of the parity line.

```{r}
RaRb_capm %>%
  ggplot(aes(x = ActivePremium)) +
  geom_histogram(binwidth = 0.005) +
  geom_vline(aes(xintercept = median(ActivePremium)), col = "red", size = 1.5)
```

## Analysis of Temporal Cluster Stability

Another question we had was whether symbols that get clustered together when looking
at the complete time frame of available data, show changing cluster membership
when looking at them in sliding windows. We decided to look at sliding windows
of three years as those are wide enough to span complete recession events (at
least in many cases).

```{r}
library(slider)

prices_index <- prices %>%
  arrange(ref.date) %>%
  (function(x) x$ref.date)

windowed_prices <- prices %>%
  tibble() %>%
  group_by(ticker) %>%
  arrange(ref.date) %>%
  slide_period(prices_index,
               "year",
               .f = (function(x) ungroup(x) %>% as_tsibble(index = ref.date, key = ticker)),
               .before = 2,
               .complete = TRUE)
```

```{r}
price_range_1999_2001 <- windowed_prices[[3]]

price_range_1999_2001 %>%
  ggplot(aes(x = ref.date, standardized_price, group = ticker)) +
  geom_line()
```

```{r}
test_window_prepped_data <- price_range_1999_2001 %>%
  features_for_price_range() %>%
  prepare_price_range(.rec = prep_rec) %>%
  extract_pca_components(pca_96_perc_rec)

test_window_clustered_data <- test_window_prepped_data %>%
  perform_clustering(.minPts = 5)

test_window_clustered_data %>%
  join_cluster_with_price_range(.price_range = price_range_1999_2001) %>%
  plot_clustered_tickers()
```

```{r, fig.width=10}
test_window_clustered_data %>%
  enrich_with_meta() %>%
  sector_freq_per_cluster()
```

```{r}
performance_noise_cluster_test_window <- get_tickers_for_cluster(test_window_clustered_data, 0) %>%
  ticker_performance_against_baseline_in_range(price_range_1999_2001)

performance_noise_cluster_test_window
```

```{r}
performance_noise_cluster_test_window %>%
  ggplot(aes(x = ActivePremium)) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = median(ActivePremium)), col = "red", size = 1.5)
```

```{r}
get_tickers_for_cluster(test_window_clustered_data, 1) %>%
  ticker_performance_against_baseline_in_range(price_range_1999_2001) %>%
  ggplot(aes(x = ActivePremium)) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = median(ActivePremium)), col = "red", size = 1.5)
```

```{r}
get_tickers_for_cluster(test_window_clustered_data, 2) %>%
  ticker_performance_against_baseline_in_range(price_range_1999_2001) %>%
  ggplot(aes(x = ActivePremium)) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = median(ActivePremium)), col = "red", size = 1.5)
```

```{r}
cluster_price_range <- function(.price_range) {
  unclustered_ticker_graph <- .price_range %>%
    ggplot(aes(x = ref.date, standardized_price, group = ticker)) +
    geom_line()
  
  price_range_prepped_data <- .price_range %>%
    features_for_price_range() %>%
    prepare_price_range(.rec = prep_rec) %>%
    extract_pca_components(pca_96_perc_rec)
  
  price_range_clustered_data <- price_range_prepped_data %>%
    perform_clustering(.minPts = 5)

  clustered_ticker_graph <- price_range_clustered_data %>%
    join_cluster_with_price_range(.price_range = .price_range) %>%
    plot_clustered_tickers()
  
  clustered_sector_composition <- price_range_clustered_data %>%
    enrich_with_meta() %>%
    sector_freq_per_cluster()
  
  list(price_range = .price_range,
       unclustered_graph = unclustered_ticker_graph,
       clustered_data = price_range_clustered_data,
       clustered_graph = clustered_ticker_graph,
       clustered_sectors = clustered_sector_composition)
}

clustered_price_windows <- prices %>%
  tibble() %>%
  group_by(ticker) %>%
  arrange(ref.date) %>%
  slide_period(prices_index,
               "year",
               .f = (function(x) ungroup(x) %>%
                       as_tsibble(index = ref.date, key = ticker) %>%
                       cluster_price_range()),
               .before = 7,
               .complete = TRUE)
```

```{r}
print_window_graphs <- function(window) {
  window$unclustered_graph %>% print()
  window$clustered_graph %>% print()
  window$clustered_sectors %>% print()
}

clustered_price_windows[[8]] %>% print_window_graphs()
```

```{r}
clustered_price_windows[[15]] %>% print_window_graphs()
```

```{r}
clustered_price_windows[[22]] %>% print_window_graphs()
```

```{r}
label_for_price_range <- function(price_range) {
  min_year <- format(price_range$ref.date %>% min(), "%Y")
  max_year <- format(price_range$ref.date %>% max(), "%Y")
  paste(min_year, "_", max_year, sep = "")
}

extract_cluster_info <- function(window_results) {
  column_label <- label_for_price_range(window_results$price_range)
  
  window_results$clustered_data %>%
    select(ticker, .cluster) %>%
    rename_with(~ column_label, .cluster)  
}

cluster_membership_over_time <- clustered_price_windows %>%
  compact() %>%
  lapply(extract_cluster_info) %>%
  reduce(inner_join, by = "ticker")

cluster_membership_over_time
```

```{r}
cluster_membership_stats <- cluster_membership_over_time %>%
  pivot_longer(cols = c(everything(), -ticker), names_to = "window", values_to = "cluster") %>%
  group_by(ticker) %>%
  summarise(memberships = length(unique(cluster)),
            outlier_count = sum(cluster == 0),
            first_count = sum(cluster == 1),
            second_count = sum(cluster == 2),
            third_count = sum(cluster == 3),
            fourth_cluster = sum(cluster == 4)) %>%
  filter(memberships > 1) %>%
  arrange(outlier_count)

cluster_membership_stats
```

```{r}
group_cluster_mode <- function(cluster_vec) {
  cluster_vec %>%
    table() %>%
    sort(decreasing = TRUE) %>%
    (function(x) x[1]) %>%
    names() %>%
    as.numeric()
}

tickers_by_all_time_cluster <- cluster_membership_over_time %>%
  pivot_longer(cols = c(everything(), -ticker), names_to = "window", values_to = "cluster") %>%
  group_by(ticker) %>%
  summarise(mode_cluster = cluster %>% group_cluster_mode())

tickers_by_all_time_cluster
```

```{r}
get_cluster_performance <- function(window_result) {
  price_period <- label_for_price_range(window_result$price_range)
  
  window_result$clustered_data$.cluster %>%
    unique() %>%
    lapply((function(x) get_tickers_for_cluster(window_result$clustered_data, x) %>%
              ticker_performance_against_baseline_in_range(window_result$price_range) %>%
              (function(y) tibble(median_active_premium = median(y$ActivePremium, na.rm = TRUE),
                                  cluster = x,
                                  period = price_period)))) %>%
    reduce(bind_rows)
}

cluster_performances_per_period <- clustered_price_windows %>%
  compact() %>%
  lapply(get_cluster_performance) %>%
  reduce(bind_rows)

cluster_performances_per_period
```

```{r}
cluster_performances_per_period %>%
  mutate(cluster = as_factor(cluster)) %>%
  ggplot(aes(x = cluster, y = median_active_premium, fill = cluster)) +
  geom_violin()
```

```{r}
cluster_of_interest <- prices %>%
  tibble() %>%
  left_join(tickers_by_all_time_cluster, by = "ticker") %>%
  left_join(ticker_meta, by = c("ticker" = "Symbol")) %>%
  filter(mode_cluster == 2) %>%
  as_tsibble(index = ref.date, key = ticker) %>%
  remove_empty(which = "cols")

cluster_of_interest
```

```{r}
cluster_of_interest %>%
  ggplot(aes(x = ref.date, standardized_price, group = ticker)) +
  geom_line()
```

```{r}
tickers_by_all_time_cluster %>%
  filter(mode_cluster == 2) %>%
  left_join(ticker_meta, by = c("ticker" = "Symbol")) %>%
  select(ticker, Name, Sector, Subsector)
```
